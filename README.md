# Tradução utilizando a arquitetura Transformer

## Pontos Positivos da Implementação

A implementação da arquitetura Transformer mostrou ser eficiente. A construção das camadas de codificação e decodificação, junto com os mecanismos de atenção, destacou a capacidade do modelo de entender melhor o contexto e as relações entre as palavras durante o treinamento.

Além disso, o tutorial usado foi útil para entender como cada parte da arquitetura funciona. Algumas partes da documentação apresentaram problemas, e a necessidade de depuração do código ajudou a compreender melhor a estrutura da Transformer.

## Pontos Negativos

Apesar da eficiência, a implementação da Transformer é complexa e leva mais tempo do que outras abordagens. 

O tutorial forneceu uma visão muito básica e, por isso, foi necessário buscar outras fontes, como o artigo original da Transformer ("Attention is All You Need") e ferramentas como o ChatGPT, para esclarecer dúvidas.

O tempo de treinamento também foi maior do que o esperado, tornando o processo mais longo e exigente. Isso torna a implementação menos prática em alguns casos, devido à demanda por mais tempo e recursos.
